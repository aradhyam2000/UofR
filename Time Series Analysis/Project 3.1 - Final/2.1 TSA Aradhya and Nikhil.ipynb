{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8aa1c9",
   "metadata": {},
   "source": [
    "## 2.1.\t(25 points) Modify the implementation of the network to leverage the RNN subclass of module torch.nn, which readily incorporates support for batch training. Note that the “nn.RNN” class is modified to operate in a batch mode.\n",
    "\n",
    "#### Submitted by:\n",
    "#### Aradhya Mathur\n",
    "#### Lakshmi Nikhil Goduguluri\n",
    "Set the hidden state size to 128 and train the network through five epochs with a batch size equal to the total number of samples. Note that, since the data samples are of different lengths, you will need to pad the length of the samples to a unique sequence length (e.g., at least the length of the longest sequence) in order to be able to feed the batch to the network.\n",
    "This is because RNN expects the input to be a tensor of shape (batch, seq_len, input_size). It is best to manually pad with 0s, or you can use built-in functions such as torch.nn.utils.rnn.pad_sequence to perform the padding. \n",
    "\n",
    "Report the accuracy yielded by this approach on the full training set after training for 5 epochs.\n",
    "\n",
    "The code below indicates the modified section of nn.RNN class. This section is included in the example script. \n",
    "\n",
    "class RNN(nn.Module): def init (self):\n",
    "super(RNN, self). init ()\n",
    "\n",
    "self.rnn = nn.RNN( input_size = INPUT_SIZE,\n",
    "hidden_size = HIDDEN_SIZE,\t\t\t# number of hidden units num_layers = N-LAYERS,\t\t# number of layers batch_first = True,\t# If your input data is of shape\n",
    "(seq_len, batch_size, features) then you don’t need batch_first=True and your RNN will output a tensor with shape (seq_len, batch.\n",
    "#If your input data is of shape (batch_size, seq_len, features) then you need batch_first=True and your RNN will output a tensor with shape (batch_size, seq_len, hidden_size).\n",
    ")\n",
    "self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "def forward(self, x):\n",
    "#r_out, (h_n, h_c) = self.rnn(x, None)\t# None represents zero initial hidden state\n",
    "r_out, h = self.rnn(x, None)\t# None represents zero initial hidden\n",
    "state\n",
    "\n",
    " choose last time step of output \n",
    "out = self.out(r_out[:, -1, :]) return out\n",
    "\n",
    "Recall that input_size refers to the size of the features (in this case one-hot encoded representation of each letter). Hidden size is a hyper parameter you can adjust (we will keep it fixed at 128 in this question). The comments in the code above explain how to format your batch data, depending on the value of batch_first. Lastly, OUTPUT_SIZE denotes the number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d822f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticke\n",
    "\n",
    "\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "\n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "# maxlen = ..... # Add code here to compute the maximum length of string \n",
    "\n",
    "maxlen = max(len(x[0]) for x in allnames)\n",
    "padded_length = maxlen\n",
    "print(padded_length)       \n",
    "                \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432e1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8916 | accuracy: 0.47\n",
      "Epoch:  1 | train loss: 2.6784 | accuracy: 0.47\n",
      "Epoch:  2 | train loss: 2.1190 | accuracy: 0.47\n",
      "Epoch:  3 | train loss: 1.9796 | accuracy: 0.47\n",
      "Epoch:  4 | train loss: 1.9510 | accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "for epoch in range(5):  \n",
    "    batch_size = len(allnames)\n",
    "    random.shuffle(allnames)\n",
    "     # if \"b_in\" and \"b_out\" are the variable names for input and output tensors, you need to create those\n",
    "    b_in = torch.zeros(batch_size, padded_length, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "    b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "    def get(charachter):\n",
    "        return [x for x in charachter]\n",
    "    \n",
    "    # (TO DO:) Populate \"b_in\" and \"b_out\" tensor. Can be done in a single loop\n",
    "    for i in allnames:\n",
    "        j=allnames.index(i)       \n",
    "        k=get(i[0])\n",
    "        for l in range(len(i[0])):\n",
    "            b_in[j][l][letterToIndex(k[l])]=1\n",
    "        m=i[1]\n",
    "        l=languages.index(m)\n",
    "        b_out[j][l]=1   \n",
    "    max_b_out=torch.max(b_out,1)[1]\n",
    "    output = rnn(b_in)                               # rnn output\n",
    "    #(TO DO:)\n",
    "    loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "    optimizer.zero_grad()                           # clear gradients for this training step\n",
    "    loss.backward()                                 # backpropagation, compute gradients\n",
    "    optimizer.step()                                # apply gradients\n",
    "         # Print accuracy\n",
    "    test_output = rnn(b_in)                   # \n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "    accuracy = sum(pred_y == test_y)/batch_size\n",
    "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4135eca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20074, 19, 57])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41f886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
