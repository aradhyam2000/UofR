{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5137ef0",
   "metadata": {},
   "source": [
    "## 2.2.\t(10 points) Modify the implementation from 2.1 to support arbitrary mini-batch sizes.\n",
    "#### Submitted by:\n",
    "#### Aradhya Mathur\n",
    "#### Lakshmi Nikhil Goduguluri\n",
    "In this case, instead of padding to a unique sequence length, adaptively pad the length of the mini batch to the length of the longest sample in the mini batch itself. Report the accuracy number (on the full training set) yielded by this approach on mini batch sizes of 1000, 2000, 3000 after five epochs of training.\n",
    "\n",
    "Note that since these problems only ask you to train for five epochs it won’t be graded based on performance (unless you get significantly smaller numbers than what’s reasonable for five epochs of training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81cd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.9527 | accuracy: 0.484\n",
      "Epoch:  0 | train loss: 2.7494 | accuracy: 0.492\n",
      "Epoch:  0 | train loss: 2.1975 | accuracy: 0.453\n",
      "Epoch:  0 | train loss: 1.9759 | accuracy: 0.460\n",
      "Epoch:  0 | train loss: 1.9487 | accuracy: 0.462\n",
      "Epoch:  0 | train loss: 1.9491 | accuracy: 0.468\n",
      "Epoch:  0 | train loss: 1.8701 | accuracy: 0.464\n",
      "Epoch:  0 | train loss: 1.9240 | accuracy: 0.443\n",
      "Epoch:  0 | train loss: 1.8639 | accuracy: 0.474\n",
      "Epoch:  0 | train loss: 1.9230 | accuracy: 0.462\n",
      "Epoch:  0 | train loss: 1.8599 | accuracy: 0.480\n",
      "Epoch:  0 | train loss: 1.9042 | accuracy: 0.461\n",
      "Epoch:  0 | train loss: 1.7810 | accuracy: 0.499\n",
      "Epoch:  0 | train loss: 1.8760 | accuracy: 0.459\n",
      "Epoch:  0 | train loss: 1.8938 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 1.8182 | accuracy: 0.487\n",
      "Epoch:  0 | train loss: 1.8589 | accuracy: 0.460\n",
      "Epoch:  0 | train loss: 1.8336 | accuracy: 0.471\n",
      "Epoch:  0 | train loss: 1.9328 | accuracy: 0.442\n",
      "Epoch:  0 | train loss: 1.8108 | accuracy: 0.485\n",
      "Epoch:  0 | train loss: 1.8901 | accuracy: 0.527\n",
      "Epoch:  1 | train loss: 1.7890 | accuracy: 0.484\n",
      "Epoch:  1 | train loss: 1.8144 | accuracy: 0.492\n",
      "Epoch:  1 | train loss: 1.9304 | accuracy: 0.453\n",
      "Epoch:  1 | train loss: 1.9041 | accuracy: 0.460\n",
      "Epoch:  1 | train loss: 1.8772 | accuracy: 0.462\n",
      "Epoch:  1 | train loss: 1.8703 | accuracy: 0.468\n",
      "Epoch:  1 | train loss: 1.8316 | accuracy: 0.464\n",
      "Epoch:  1 | train loss: 1.9191 | accuracy: 0.443\n",
      "Epoch:  1 | train loss: 1.8366 | accuracy: 0.474\n",
      "Epoch:  1 | train loss: 1.8894 | accuracy: 0.462\n",
      "Epoch:  1 | train loss: 1.8109 | accuracy: 0.480\n",
      "Epoch:  1 | train loss: 1.8959 | accuracy: 0.461\n",
      "Epoch:  1 | train loss: 1.7770 | accuracy: 0.499\n",
      "Epoch:  1 | train loss: 1.8699 | accuracy: 0.459\n",
      "Epoch:  1 | train loss: 1.8811 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 1.8020 | accuracy: 0.487\n",
      "Epoch:  1 | train loss: 1.8513 | accuracy: 0.460\n",
      "Epoch:  1 | train loss: 1.8351 | accuracy: 0.471\n",
      "Epoch:  1 | train loss: 1.9144 | accuracy: 0.442\n",
      "Epoch:  1 | train loss: 1.8024 | accuracy: 0.485\n",
      "Epoch:  1 | train loss: 1.8814 | accuracy: 0.527\n",
      "Epoch:  2 | train loss: 1.7901 | accuracy: 0.484\n",
      "Epoch:  2 | train loss: 1.8160 | accuracy: 0.492\n",
      "Epoch:  2 | train loss: 1.9282 | accuracy: 0.453\n",
      "Epoch:  2 | train loss: 1.9004 | accuracy: 0.460\n",
      "Epoch:  2 | train loss: 1.8778 | accuracy: 0.462\n",
      "Epoch:  2 | train loss: 1.8754 | accuracy: 0.468\n",
      "Epoch:  2 | train loss: 1.8371 | accuracy: 0.464\n",
      "Epoch:  2 | train loss: 1.9246 | accuracy: 0.443\n",
      "Epoch:  2 | train loss: 1.8467 | accuracy: 0.474\n",
      "Epoch:  2 | train loss: 1.8930 | accuracy: 0.462\n",
      "Epoch:  2 | train loss: 1.8190 | accuracy: 0.480\n",
      "Epoch:  2 | train loss: 1.8902 | accuracy: 0.461\n",
      "Epoch:  2 | train loss: 1.7739 | accuracy: 0.499\n",
      "Epoch:  2 | train loss: 1.8679 | accuracy: 0.459\n",
      "Epoch:  2 | train loss: 1.8852 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 1.8076 | accuracy: 0.487\n",
      "Epoch:  2 | train loss: 1.8543 | accuracy: 0.460\n",
      "Epoch:  2 | train loss: 1.8272 | accuracy: 0.471\n",
      "Epoch:  2 | train loss: 1.9137 | accuracy: 0.442\n",
      "Epoch:  2 | train loss: 1.8052 | accuracy: 0.485\n",
      "Epoch:  2 | train loss: 1.8949 | accuracy: 0.527\n",
      "Epoch:  3 | train loss: 1.8005 | accuracy: 0.484\n",
      "Epoch:  3 | train loss: 1.8206 | accuracy: 0.492\n",
      "Epoch:  3 | train loss: 1.9188 | accuracy: 0.453\n",
      "Epoch:  3 | train loss: 1.8945 | accuracy: 0.460\n",
      "Epoch:  3 | train loss: 1.8742 | accuracy: 0.462\n",
      "Epoch:  3 | train loss: 1.8831 | accuracy: 0.468\n",
      "Epoch:  3 | train loss: 1.8492 | accuracy: 0.464\n",
      "Epoch:  3 | train loss: 1.9171 | accuracy: 0.443\n",
      "Epoch:  3 | train loss: 1.8379 | accuracy: 0.474\n",
      "Epoch:  3 | train loss: 1.8858 | accuracy: 0.462\n",
      "Epoch:  3 | train loss: 1.8198 | accuracy: 0.480\n",
      "Epoch:  3 | train loss: 1.8992 | accuracy: 0.461\n",
      "Epoch:  3 | train loss: 1.7747 | accuracy: 0.499\n",
      "Epoch:  3 | train loss: 1.8657 | accuracy: 0.459\n",
      "Epoch:  3 | train loss: 1.8767 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 1.8034 | accuracy: 0.487\n",
      "Epoch:  3 | train loss: 1.8562 | accuracy: 0.460\n",
      "Epoch:  3 | train loss: 1.8338 | accuracy: 0.471\n",
      "Epoch:  3 | train loss: 1.9197 | accuracy: 0.442\n",
      "Epoch:  3 | train loss: 1.8061 | accuracy: 0.485\n",
      "Epoch:  3 | train loss: 1.8763 | accuracy: 0.527\n",
      "Epoch:  4 | train loss: 1.7956 | accuracy: 0.484\n",
      "Epoch:  4 | train loss: 1.8220 | accuracy: 0.492\n",
      "Epoch:  4 | train loss: 1.9156 | accuracy: 0.453\n",
      "Epoch:  4 | train loss: 1.8913 | accuracy: 0.460\n",
      "Epoch:  4 | train loss: 1.8599 | accuracy: 0.462\n",
      "Epoch:  4 | train loss: 1.8720 | accuracy: 0.468\n",
      "Epoch:  4 | train loss: 1.8398 | accuracy: 0.464\n",
      "Epoch:  4 | train loss: 1.9186 | accuracy: 0.443\n",
      "Epoch:  4 | train loss: 1.8412 | accuracy: 0.474\n",
      "Epoch:  4 | train loss: 1.8864 | accuracy: 0.462\n",
      "Epoch:  4 | train loss: 1.8085 | accuracy: 0.480\n",
      "Epoch:  4 | train loss: 1.8916 | accuracy: 0.461\n",
      "Epoch:  4 | train loss: 1.7725 | accuracy: 0.499\n",
      "Epoch:  4 | train loss: 1.8652 | accuracy: 0.459\n",
      "Epoch:  4 | train loss: 1.8768 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 1.7997 | accuracy: 0.487\n",
      "Epoch:  4 | train loss: 1.8497 | accuracy: 0.460\n",
      "Epoch:  4 | train loss: 1.8318 | accuracy: 0.471\n",
      "Epoch:  4 | train loss: 1.9160 | accuracy: 0.442\n",
      "Epoch:  4 | train loss: 1.8053 | accuracy: 0.485\n",
      "Epoch:  4 | train loss: 1.8712 | accuracy: 0.527\n",
      "Average accuracy is 0.47061904761904766\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "random.shuffle(allnames)\n",
    "n = 1000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(batch_in_all_names)\n",
    "        \n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [z for z in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(\"Average accuracy is\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa384ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.9143 | accuracy: 0.484\n",
      "Epoch:  0 | train loss: 2.7362 | accuracy: 0.482\n",
      "Epoch:  0 | train loss: 2.3330 | accuracy: 0.465\n",
      "Epoch:  0 | train loss: 2.0021 | accuracy: 0.454\n",
      "Epoch:  0 | train loss: 1.9107 | accuracy: 0.465\n",
      "Epoch:  0 | train loss: 1.9316 | accuracy: 0.465\n",
      "Epoch:  0 | train loss: 1.9261 | accuracy: 0.456\n",
      "Epoch:  0 | train loss: 1.8570 | accuracy: 0.469\n",
      "Epoch:  0 | train loss: 1.8987 | accuracy: 0.464\n",
      "Epoch:  0 | train loss: 1.9000 | accuracy: 0.470\n",
      "Epoch:  0 | train loss: 1.6729 | accuracy: 0.514\n",
      "Epoch:  1 | train loss: 1.8462 | accuracy: 0.484\n",
      "Epoch:  1 | train loss: 1.7928 | accuracy: 0.482\n",
      "Epoch:  1 | train loss: 1.8641 | accuracy: 0.465\n",
      "Epoch:  1 | train loss: 1.9122 | accuracy: 0.454\n",
      "Epoch:  1 | train loss: 1.8689 | accuracy: 0.465\n",
      "Epoch:  1 | train loss: 1.8910 | accuracy: 0.465\n",
      "Epoch:  1 | train loss: 1.8837 | accuracy: 0.456\n",
      "Epoch:  1 | train loss: 1.8304 | accuracy: 0.469\n",
      "Epoch:  1 | train loss: 1.8824 | accuracy: 0.464\n",
      "Epoch:  1 | train loss: 1.8809 | accuracy: 0.470\n",
      "Epoch:  1 | train loss: 1.6824 | accuracy: 0.514\n",
      "Epoch:  2 | train loss: 1.8184 | accuracy: 0.484\n",
      "Epoch:  2 | train loss: 1.7934 | accuracy: 0.482\n",
      "Epoch:  2 | train loss: 1.8586 | accuracy: 0.465\n",
      "Epoch:  2 | train loss: 1.8977 | accuracy: 0.454\n",
      "Epoch:  2 | train loss: 1.8450 | accuracy: 0.465\n",
      "Epoch:  2 | train loss: 1.8863 | accuracy: 0.465\n",
      "Epoch:  2 | train loss: 1.8950 | accuracy: 0.456\n",
      "Epoch:  2 | train loss: 1.8313 | accuracy: 0.469\n",
      "Epoch:  2 | train loss: 1.8709 | accuracy: 0.464\n",
      "Epoch:  2 | train loss: 1.8715 | accuracy: 0.470\n",
      "Epoch:  2 | train loss: 1.6504 | accuracy: 0.514\n",
      "Epoch:  3 | train loss: 1.8246 | accuracy: 0.484\n",
      "Epoch:  3 | train loss: 1.7986 | accuracy: 0.482\n",
      "Epoch:  3 | train loss: 1.8634 | accuracy: 0.465\n",
      "Epoch:  3 | train loss: 1.8940 | accuracy: 0.454\n",
      "Epoch:  3 | train loss: 1.8426 | accuracy: 0.465\n",
      "Epoch:  3 | train loss: 1.8752 | accuracy: 0.465\n",
      "Epoch:  3 | train loss: 1.8861 | accuracy: 0.456\n",
      "Epoch:  3 | train loss: 1.8275 | accuracy: 0.469\n",
      "Epoch:  3 | train loss: 1.8756 | accuracy: 0.464\n",
      "Epoch:  3 | train loss: 1.8714 | accuracy: 0.470\n",
      "Epoch:  3 | train loss: 1.6499 | accuracy: 0.514\n",
      "Epoch:  4 | train loss: 1.8260 | accuracy: 0.484\n",
      "Epoch:  4 | train loss: 1.8012 | accuracy: 0.482\n",
      "Epoch:  4 | train loss: 1.8659 | accuracy: 0.465\n",
      "Epoch:  4 | train loss: 1.9004 | accuracy: 0.454\n",
      "Epoch:  4 | train loss: 1.8442 | accuracy: 0.465\n",
      "Epoch:  4 | train loss: 1.8773 | accuracy: 0.465\n",
      "Epoch:  4 | train loss: 1.8850 | accuracy: 0.456\n",
      "Epoch:  4 | train loss: 1.8289 | accuracy: 0.469\n",
      "Epoch:  4 | train loss: 1.8806 | accuracy: 0.464\n",
      "Epoch:  4 | train loss: 1.8766 | accuracy: 0.470\n",
      "Epoch:  4 | train loss: 1.6750 | accuracy: 0.514\n",
      "Average accuracy is 0.47154545454545443\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "random.shuffle(allnames)\n",
    "n = 2000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(batch_in_all_names)\n",
    "        \n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [z for z in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(\"Average accuracy is\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3845dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8482 | accuracy: 0.472\n",
      "Epoch:  0 | train loss: 2.6521 | accuracy: 0.469\n",
      "Epoch:  0 | train loss: 2.1873 | accuracy: 0.464\n",
      "Epoch:  0 | train loss: 1.9353 | accuracy: 0.480\n",
      "Epoch:  0 | train loss: 1.9598 | accuracy: 0.463\n",
      "Epoch:  0 | train loss: 1.9508 | accuracy: 0.465\n",
      "Epoch:  0 | train loss: 1.8793 | accuracy: 0.456\n",
      "Epoch:  1 | train loss: 1.8638 | accuracy: 0.472\n",
      "Epoch:  1 | train loss: 1.8731 | accuracy: 0.469\n",
      "Epoch:  1 | train loss: 1.9056 | accuracy: 0.464\n",
      "Epoch:  1 | train loss: 1.8671 | accuracy: 0.480\n",
      "Epoch:  1 | train loss: 1.8795 | accuracy: 0.463\n",
      "Epoch:  1 | train loss: 1.8695 | accuracy: 0.465\n",
      "Epoch:  1 | train loss: 1.8463 | accuracy: 0.456\n",
      "Epoch:  2 | train loss: 1.8714 | accuracy: 0.472\n",
      "Epoch:  2 | train loss: 1.8596 | accuracy: 0.469\n",
      "Epoch:  2 | train loss: 1.8758 | accuracy: 0.464\n",
      "Epoch:  2 | train loss: 1.8486 | accuracy: 0.480\n",
      "Epoch:  2 | train loss: 1.8644 | accuracy: 0.463\n",
      "Epoch:  2 | train loss: 1.8664 | accuracy: 0.465\n",
      "Epoch:  2 | train loss: 1.8535 | accuracy: 0.456\n",
      "Epoch:  3 | train loss: 1.8556 | accuracy: 0.472\n",
      "Epoch:  3 | train loss: 1.8451 | accuracy: 0.469\n",
      "Epoch:  3 | train loss: 1.8607 | accuracy: 0.464\n",
      "Epoch:  3 | train loss: 1.8403 | accuracy: 0.480\n",
      "Epoch:  3 | train loss: 1.8548 | accuracy: 0.463\n",
      "Epoch:  3 | train loss: 1.8584 | accuracy: 0.465\n",
      "Epoch:  3 | train loss: 1.8418 | accuracy: 0.456\n",
      "Epoch:  4 | train loss: 1.8510 | accuracy: 0.472\n",
      "Epoch:  4 | train loss: 1.8378 | accuracy: 0.469\n",
      "Epoch:  4 | train loss: 1.8532 | accuracy: 0.464\n",
      "Epoch:  4 | train loss: 1.8363 | accuracy: 0.480\n",
      "Epoch:  4 | train loss: 1.8546 | accuracy: 0.463\n",
      "Epoch:  4 | train loss: 1.8607 | accuracy: 0.465\n",
      "Epoch:  4 | train loss: 1.8421 | accuracy: 0.456\n",
      "Average accuracy is 0.46699999999999997\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "random.shuffle(allnames)\n",
    "n = 3000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(batch_in_all_names)\n",
    "        \n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [z for z in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(\"Average accuracy is\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f11bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc3bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3c6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
