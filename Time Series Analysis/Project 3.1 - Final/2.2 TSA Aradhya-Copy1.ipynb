{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44f4b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.9395 | accuracy: 0.500\n",
      "Epoch:  0 | train loss: 2.4891 | accuracy: 0.500\n",
      "Epoch:  0 | train loss: 1.4418 | accuracy: 0.476\n",
      "Epoch:  0 | train loss: 0.4789 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3472 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3316 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3393 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4065 | accuracy: 0.476\n",
      "Epoch:  0 | train loss: 0.4365 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4874 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4755 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4649 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4366 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.4097 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3782 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3429 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.3077 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.2701 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.2366 | accuracy: 0.476\n",
      "Epoch:  0 | train loss: 0.3417 | accuracy: 0.475\n",
      "Epoch:  0 | train loss: 0.1366 | accuracy: 0.498\n",
      "Epoch:  1 | train loss: 0.1159 | accuracy: 0.500\n",
      "Epoch:  1 | train loss: 0.0959 | accuracy: 0.500\n",
      "Epoch:  1 | train loss: 0.5090 | accuracy: 0.476\n",
      "Epoch:  1 | train loss: 0.3795 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.3720 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.3691 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.3834 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.4105 | accuracy: 0.476\n",
      "Epoch:  1 | train loss: 0.4113 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.4001 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.3225 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2837 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2787 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2738 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2656 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2554 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2467 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2378 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.2314 | accuracy: 0.476\n",
      "Epoch:  1 | train loss: 0.3343 | accuracy: 0.475\n",
      "Epoch:  1 | train loss: 0.1403 | accuracy: 0.498\n",
      "Epoch:  2 | train loss: 0.1040 | accuracy: 0.500\n",
      "Epoch:  2 | train loss: 0.0819 | accuracy: 0.500\n",
      "Epoch:  2 | train loss: 0.4175 | accuracy: 0.476\n",
      "Epoch:  2 | train loss: 0.3285 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.3207 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.3155 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.3351 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.3927 | accuracy: 0.476\n",
      "Epoch:  2 | train loss: 0.3972 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.4030 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.3273 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.2975 | accuracy: 0.475\n",
      "Epoch:  2 | train loss: 0.2945 | accuracy: 0.475\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_59596\\1754602680.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# Print accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_in\u001b[0m\u001b[1;33m)\u001b[0m                   \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_59596\\1754602680.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mr_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# None represents zero initial hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RNN_TANH'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m                 result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    477\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                                       self.batch_first)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "\n",
    "n = 1000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(allnames)\n",
    "        random.shuffle(allnames)\n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [char for char in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(2*batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(np.mean(accuracies))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3bbcd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f6c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8151 | accuracy: 10.037\n",
      "Epoch:  0 | train loss: 2.4356 | accuracy: 9.062\n",
      "Epoch:  0 | train loss: 1.4394 | accuracy: 9.037\n",
      "Epoch:  0 | train loss: 0.6521 | accuracy: 9.061\n",
      "Epoch:  0 | train loss: 0.6645 | accuracy: 9.044\n",
      "Epoch:  0 | train loss: 0.6180 | accuracy: 9.038\n",
      "Epoch:  0 | train loss: 0.5982 | accuracy: 9.046\n",
      "Epoch:  0 | train loss: 0.5649 | accuracy: 9.040\n",
      "Epoch:  0 | train loss: 0.5116 | accuracy: 9.041\n",
      "Epoch:  0 | train loss: 0.5360 | accuracy: 9.055\n",
      "Epoch:  0 | train loss: 0.0696 | accuracy: 270.297\n",
      "Epoch:  1 | train loss: 0.0594 | accuracy: 10.037\n",
      "Epoch:  1 | train loss: 0.8191 | accuracy: 9.062\n",
      "Epoch:  1 | train loss: 0.6978 | accuracy: 9.037\n",
      "Epoch:  1 | train loss: 0.7533 | accuracy: 9.061\n",
      "Epoch:  1 | train loss: 0.7378 | accuracy: 9.044\n",
      "Epoch:  1 | train loss: 0.4044 | accuracy: 9.038\n",
      "Epoch:  1 | train loss: 0.3570 | accuracy: 9.046\n",
      "Epoch:  1 | train loss: 0.3660 | accuracy: 9.040\n",
      "Epoch:  1 | train loss: 0.3703 | accuracy: 9.041\n",
      "Epoch:  1 | train loss: 0.4718 | accuracy: 9.055\n",
      "Epoch:  1 | train loss: 0.1580 | accuracy: 270.297\n",
      "Epoch:  2 | train loss: 0.1071 | accuracy: 10.037\n",
      "Epoch:  2 | train loss: 0.6822 | accuracy: 9.062\n",
      "Epoch:  2 | train loss: 0.4952 | accuracy: 9.037\n",
      "Epoch:  2 | train loss: 0.6354 | accuracy: 9.061\n",
      "Epoch:  2 | train loss: 0.6755 | accuracy: 9.044\n",
      "Epoch:  2 | train loss: 0.5000 | accuracy: 9.038\n",
      "Epoch:  2 | train loss: 0.4583 | accuracy: 9.046\n",
      "Epoch:  2 | train loss: 0.4564 | accuracy: 9.040\n",
      "Epoch:  2 | train loss: 0.4429 | accuracy: 9.041\n",
      "Epoch:  2 | train loss: 0.4929 | accuracy: 9.055\n",
      "Epoch:  2 | train loss: 0.1417 | accuracy: 270.297\n",
      "Epoch:  3 | train loss: 0.1156 | accuracy: 10.037\n",
      "Epoch:  3 | train loss: 0.6158 | accuracy: 9.062\n",
      "Epoch:  3 | train loss: 0.5060 | accuracy: 9.037\n",
      "Epoch:  3 | train loss: 0.6142 | accuracy: 9.061\n",
      "Epoch:  3 | train loss: 0.6561 | accuracy: 9.044\n",
      "Epoch:  3 | train loss: 0.4522 | accuracy: 9.038\n",
      "Epoch:  3 | train loss: 0.4090 | accuracy: 9.046\n",
      "Epoch:  3 | train loss: 0.4086 | accuracy: 9.040\n",
      "Epoch:  3 | train loss: 0.4002 | accuracy: 9.041\n",
      "Epoch:  3 | train loss: 0.4593 | accuracy: 9.055\n",
      "Epoch:  3 | train loss: 0.1425 | accuracy: 270.297\n",
      "Epoch:  4 | train loss: 0.1101 | accuracy: 10.037\n",
      "Epoch:  4 | train loss: 0.6226 | accuracy: 9.062\n",
      "Epoch:  4 | train loss: 0.5466 | accuracy: 9.037\n",
      "Epoch:  4 | train loss: 0.6333 | accuracy: 9.061\n",
      "Epoch:  4 | train loss: 0.6835 | accuracy: 9.044\n",
      "Epoch:  4 | train loss: 0.4528 | accuracy: 9.038\n",
      "Epoch:  4 | train loss: 0.4123 | accuracy: 9.046\n",
      "Epoch:  4 | train loss: 0.4117 | accuracy: 9.040\n",
      "Epoch:  4 | train loss: 0.4027 | accuracy: 9.041\n",
      "Epoch:  4 | train loss: 0.4643 | accuracy: 9.055\n",
      "Epoch:  4 | train loss: 0.1431 | accuracy: 270.297\n",
      "32.88718181818182\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "\n",
    "#def randomChoice(l, size):\n",
    " #   h=[]\n",
    "  #  random.shuffle(l)\n",
    "   # for i in range(size):\n",
    "    #    h.append(l[random.randint(0, len(l) - 1)])\n",
    "        \n",
    "   # return h\n",
    "\n",
    "# allnames1000=randomChoice(allnames, 1000)\n",
    "\n",
    "n = 2000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    \n",
    "    for batch_in_all_names in x:\n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        random.shuffle(batch_in_all_names)\n",
    "        batch_size = len(batch_in_all_names)\n",
    "        \n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        def split(word):\n",
    "            return [char for char in word]\n",
    "        # (TO DO:) Populate \"b_in\" tensor\n",
    "        for name in batch_in_all_names:\n",
    "            i=batch_in_all_names.index(name)       \n",
    "            list1=split(name[0])\n",
    "            for m in range(len(name[0])):\n",
    "                b_in[i][m][letterToIndex(list1[m])]=1\n",
    "\n",
    "\n",
    "\n",
    "        # (TO DO:) Populate \"b_out\" tensor\n",
    "        for name in batch_in_all_names:       \n",
    "            i=batch_in_all_names.index(name)       \n",
    "            lan=name[1]\n",
    "            l=languages.index(lan)\n",
    "            b_out[i][l]=1\n",
    "\n",
    "\n",
    "        labels=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, labels)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/batch_size\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "print(np.mean(accuracies))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b28ac2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1bc7a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8179 | accuracy: 0.500\n",
      "Epoch:  0 | train loss: 2.3750 | accuracy: 0.451\n",
      "Epoch:  0 | train loss: 1.3329 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 0.6214 | accuracy: 0.451\n",
      "Epoch:  0 | train loss: 0.6021 | accuracy: 0.451\n",
      "Epoch:  0 | train loss: 0.7154 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 0.6970 | accuracy: 0.451\n",
      "Epoch:  0 | train loss: 0.6546 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 0.5886 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 0.5733 | accuracy: 0.451\n",
      "Epoch:  0 | train loss: 0.0655 | accuracy: 0.498\n",
      "Epoch:  1 | train loss: 0.0523 | accuracy: 0.500\n",
      "Epoch:  1 | train loss: 0.7298 | accuracy: 0.451\n",
      "Epoch:  1 | train loss: 0.5546 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 0.7008 | accuracy: 0.451\n",
      "Epoch:  1 | train loss: 0.7181 | accuracy: 0.451\n",
      "Epoch:  1 | train loss: 0.4232 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 0.3756 | accuracy: 0.451\n",
      "Epoch:  1 | train loss: 0.3815 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 0.3799 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 0.4706 | accuracy: 0.451\n",
      "Epoch:  1 | train loss: 0.1488 | accuracy: 0.498\n",
      "Epoch:  2 | train loss: 0.0948 | accuracy: 0.500\n",
      "Epoch:  2 | train loss: 0.6842 | accuracy: 0.451\n",
      "Epoch:  2 | train loss: 0.5310 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 0.6585 | accuracy: 0.451\n",
      "Epoch:  2 | train loss: 0.6921 | accuracy: 0.451\n",
      "Epoch:  2 | train loss: 0.4897 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 0.4427 | accuracy: 0.451\n",
      "Epoch:  2 | train loss: 0.4360 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 0.4197 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 0.4742 | accuracy: 0.451\n",
      "Epoch:  2 | train loss: 0.1549 | accuracy: 0.498\n",
      "Epoch:  3 | train loss: 0.1364 | accuracy: 0.500\n",
      "Epoch:  3 | train loss: 0.6216 | accuracy: 0.451\n",
      "Epoch:  3 | train loss: 0.5147 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 0.6099 | accuracy: 0.451\n",
      "Epoch:  3 | train loss: 0.6608 | accuracy: 0.451\n",
      "Epoch:  3 | train loss: 0.4576 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 0.4201 | accuracy: 0.451\n",
      "Epoch:  3 | train loss: 0.4230 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 0.4156 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 0.4708 | accuracy: 0.451\n",
      "Epoch:  3 | train loss: 0.1287 | accuracy: 0.498\n",
      "Epoch:  4 | train loss: 0.0975 | accuracy: 0.500\n",
      "Epoch:  4 | train loss: 0.6100 | accuracy: 0.451\n",
      "Epoch:  4 | train loss: 0.5247 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 0.6252 | accuracy: 0.451\n",
      "Epoch:  4 | train loss: 0.6793 | accuracy: 0.451\n",
      "Epoch:  4 | train loss: 0.4509 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 0.4093 | accuracy: 0.451\n",
      "Epoch:  4 | train loss: 0.4080 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 0.3993 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 0.4619 | accuracy: 0.451\n",
      "Epoch:  4 | train loss: 0.1526 | accuracy: 0.498\n",
      "0.45936363636363636\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "\n",
    "n = 2000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(allnames)\n",
    "        random.shuffle(allnames)\n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [char for char in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(2*batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(np.mean(accuracies))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ae02977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.8500 | accuracy: 0.450\n",
      "Epoch:  0 | train loss: 2.6624 | accuracy: 0.479\n",
      "Epoch:  0 | train loss: 2.2322 | accuracy: 0.477\n",
      "Epoch:  0 | train loss: 2.0009 | accuracy: 0.470\n",
      "Epoch:  0 | train loss: 1.9274 | accuracy: 0.466\n",
      "Epoch:  0 | train loss: 1.9131 | accuracy: 0.193\n",
      "Epoch:  0 | train loss: 1.9722 | accuracy: 0.468\n",
      "Epoch:  0 | train loss: 2.0243 | accuracy: 0.457\n",
      "Epoch:  0 | train loss: 1.8797 | accuracy: 0.476\n",
      "Epoch:  0 | train loss: 1.8682 | accuracy: 0.454\n",
      "Epoch:  0 | train loss: 1.9166 | accuracy: 0.462\n",
      "Epoch:  0 | train loss: 1.9533 | accuracy: 0.455\n",
      "Epoch:  0 | train loss: 1.8365 | accuracy: 0.477\n",
      "Epoch:  0 | train loss: 1.8276 | accuracy: 0.483\n",
      "Epoch:  0 | train loss: 1.8732 | accuracy: 0.465\n",
      "Epoch:  0 | train loss: 1.8487 | accuracy: 0.476\n",
      "Epoch:  0 | train loss: 1.8156 | accuracy: 0.474\n",
      "Epoch:  0 | train loss: 1.9387 | accuracy: 0.460\n",
      "Epoch:  0 | train loss: 1.8831 | accuracy: 0.480\n",
      "Epoch:  0 | train loss: 1.7571 | accuracy: 0.494\n",
      "Epoch:  0 | train loss: 2.0375 | accuracy: 0.392\n",
      "Epoch:  1 | train loss: 1.9094 | accuracy: 0.450\n",
      "Epoch:  1 | train loss: 1.8183 | accuracy: 0.479\n",
      "Epoch:  1 | train loss: 1.8497 | accuracy: 0.477\n",
      "Epoch:  1 | train loss: 1.8479 | accuracy: 0.470\n",
      "Epoch:  1 | train loss: 1.8365 | accuracy: 0.466\n",
      "Epoch:  1 | train loss: 1.8936 | accuracy: 0.442\n",
      "Epoch:  1 | train loss: 1.8669 | accuracy: 0.468\n",
      "Epoch:  1 | train loss: 1.9555 | accuracy: 0.457\n",
      "Epoch:  1 | train loss: 1.8650 | accuracy: 0.476\n",
      "Epoch:  1 | train loss: 1.8324 | accuracy: 0.454\n",
      "Epoch:  1 | train loss: 1.8782 | accuracy: 0.462\n",
      "Epoch:  1 | train loss: 1.9155 | accuracy: 0.455\n",
      "Epoch:  1 | train loss: 1.7939 | accuracy: 0.477\n",
      "Epoch:  1 | train loss: 1.8133 | accuracy: 0.483\n",
      "Epoch:  1 | train loss: 1.8698 | accuracy: 0.465\n",
      "Epoch:  1 | train loss: 1.8359 | accuracy: 0.476\n",
      "Epoch:  1 | train loss: 1.8124 | accuracy: 0.474\n",
      "Epoch:  1 | train loss: 1.9175 | accuracy: 0.460\n",
      "Epoch:  1 | train loss: 1.8639 | accuracy: 0.480\n",
      "Epoch:  1 | train loss: 1.7585 | accuracy: 0.494\n",
      "Epoch:  1 | train loss: 2.0115 | accuracy: 0.392\n",
      "Epoch:  2 | train loss: 1.9009 | accuracy: 0.450\n",
      "Epoch:  2 | train loss: 1.8160 | accuracy: 0.479\n",
      "Epoch:  2 | train loss: 1.8351 | accuracy: 0.477\n",
      "Epoch:  2 | train loss: 1.8474 | accuracy: 0.470\n",
      "Epoch:  2 | train loss: 1.8384 | accuracy: 0.466\n",
      "Epoch:  2 | train loss: 1.8868 | accuracy: 0.442\n",
      "Epoch:  2 | train loss: 1.8695 | accuracy: 0.468\n",
      "Epoch:  2 | train loss: 1.9520 | accuracy: 0.457\n",
      "Epoch:  2 | train loss: 1.8605 | accuracy: 0.476\n",
      "Epoch:  2 | train loss: 1.8392 | accuracy: 0.454\n",
      "Epoch:  2 | train loss: 1.8870 | accuracy: 0.462\n",
      "Epoch:  2 | train loss: 1.9155 | accuracy: 0.455\n",
      "Epoch:  2 | train loss: 1.7861 | accuracy: 0.477\n",
      "Epoch:  2 | train loss: 1.8051 | accuracy: 0.483\n",
      "Epoch:  2 | train loss: 1.8642 | accuracy: 0.465\n",
      "Epoch:  2 | train loss: 1.8314 | accuracy: 0.476\n",
      "Epoch:  2 | train loss: 1.8142 | accuracy: 0.474\n",
      "Epoch:  2 | train loss: 1.9127 | accuracy: 0.460\n",
      "Epoch:  2 | train loss: 1.8574 | accuracy: 0.480\n",
      "Epoch:  2 | train loss: 1.7507 | accuracy: 0.494\n",
      "Epoch:  2 | train loss: 2.0006 | accuracy: 0.392\n",
      "Epoch:  3 | train loss: 1.9054 | accuracy: 0.450\n",
      "Epoch:  3 | train loss: 1.8169 | accuracy: 0.479\n",
      "Epoch:  3 | train loss: 1.8279 | accuracy: 0.477\n",
      "Epoch:  3 | train loss: 1.8399 | accuracy: 0.470\n",
      "Epoch:  3 | train loss: 1.8316 | accuracy: 0.466\n",
      "Epoch:  3 | train loss: 1.8811 | accuracy: 0.442\n",
      "Epoch:  3 | train loss: 1.8639 | accuracy: 0.468\n",
      "Epoch:  3 | train loss: 1.9492 | accuracy: 0.457\n",
      "Epoch:  3 | train loss: 1.8585 | accuracy: 0.476\n",
      "Epoch:  3 | train loss: 1.8369 | accuracy: 0.454\n",
      "Epoch:  3 | train loss: 1.8837 | accuracy: 0.462\n",
      "Epoch:  3 | train loss: 1.9141 | accuracy: 0.455\n",
      "Epoch:  3 | train loss: 1.7883 | accuracy: 0.477\n",
      "Epoch:  3 | train loss: 1.8079 | accuracy: 0.483\n",
      "Epoch:  3 | train loss: 1.8657 | accuracy: 0.465\n",
      "Epoch:  3 | train loss: 1.8275 | accuracy: 0.476\n",
      "Epoch:  3 | train loss: 1.8108 | accuracy: 0.474\n",
      "Epoch:  3 | train loss: 1.9105 | accuracy: 0.460\n",
      "Epoch:  3 | train loss: 1.8557 | accuracy: 0.480\n",
      "Epoch:  3 | train loss: 1.7494 | accuracy: 0.494\n",
      "Epoch:  3 | train loss: 1.9887 | accuracy: 0.392\n",
      "Epoch:  4 | train loss: 1.9030 | accuracy: 0.450\n",
      "Epoch:  4 | train loss: 1.8158 | accuracy: 0.479\n",
      "Epoch:  4 | train loss: 1.8303 | accuracy: 0.477\n",
      "Epoch:  4 | train loss: 1.8435 | accuracy: 0.470\n",
      "Epoch:  4 | train loss: 1.8338 | accuracy: 0.466\n",
      "Epoch:  4 | train loss: 1.8826 | accuracy: 0.442\n",
      "Epoch:  4 | train loss: 1.8638 | accuracy: 0.468\n",
      "Epoch:  4 | train loss: 1.9496 | accuracy: 0.457\n",
      "Epoch:  4 | train loss: 1.8591 | accuracy: 0.476\n",
      "Epoch:  4 | train loss: 1.8362 | accuracy: 0.454\n",
      "Epoch:  4 | train loss: 1.8825 | accuracy: 0.462\n",
      "Epoch:  4 | train loss: 1.9132 | accuracy: 0.455\n",
      "Epoch:  4 | train loss: 1.7867 | accuracy: 0.477\n",
      "Epoch:  4 | train loss: 1.8083 | accuracy: 0.483\n",
      "Epoch:  4 | train loss: 1.8644 | accuracy: 0.465\n",
      "Epoch:  4 | train loss: 1.8279 | accuracy: 0.476\n",
      "Epoch:  4 | train loss: 1.8115 | accuracy: 0.474\n",
      "Epoch:  4 | train loss: 1.9098 | accuracy: 0.460\n",
      "Epoch:  4 | train loss: 1.8559 | accuracy: 0.480\n",
      "Epoch:  4 | train loss: 1.7484 | accuracy: 0.494\n",
      "Epoch:  4 | train loss: 1.9793 | accuracy: 0.392\n",
      "0.4622476190476191\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:q\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles(r\"C:\\Users\\aradh\\Desktop\\Fall 22\\TSA\\Project 3.1\\data\\data\\names\\*.txt\"):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#list comprehension:\n",
    "# list_data=[]\n",
    "# for category in languages:\n",
    "#     for name in names[category]:\n",
    "#         list_data.append((name, category))\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "random.shuffle(allnames)\n",
    "n = 1000\n",
    "x = [allnames[i:i + n] for i in range(0, len(allnames), n)]\n",
    "#print(x)\n",
    "    \n",
    "# for category in list_data:\n",
    "#     for name in names[category]:\n",
    "#         allnames.append([name, category])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) \n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string        \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "accuracies = []\n",
    "for epoch in range(5):  \n",
    "    for batch_in_all_names in x:\n",
    "        batch_size = len(batch_in_all_names)\n",
    "        \n",
    "        maxlen = max(len(x[0]) for x in batch_in_all_names)\n",
    "        b_in = torch.zeros(batch_size, maxlen, n_letters)  # (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "        b_out = torch.zeros(batch_size, n_categories, dtype=torch.long)  # (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "        \n",
    "        def get(charachter):\n",
    "            return [char for char in charachter]\n",
    "        for i in batch_in_all_names:\n",
    "            j=batch_in_all_names.index(i)       \n",
    "            k=get(i[0])\n",
    "            for l in range(len(i[0])):\n",
    "                b_in[j][l][letterToIndex(k[l])]=1\n",
    "            m=i[1]\n",
    "            l=languages.index(m)\n",
    "            b_out[j][l]=1   \n",
    "        max_b_out=torch.max(b_out,1)[1]\n",
    "        output = rnn(b_in)                               # rnn output\n",
    "        #(TO DO:)\n",
    "        loss = loss_func(output, max_b_out)   # (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        # Print accuracy\n",
    "        test_output = rnn(b_in)                   # \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = sum(pred_y == test_y)/(batch_size)\n",
    "        print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.3f' % accuracy)\n",
    "        accuracies.append(round(accuracy,3))\n",
    "\n",
    "print(np.mean(accuracies))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab922b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
